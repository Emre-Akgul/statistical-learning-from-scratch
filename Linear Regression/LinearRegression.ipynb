{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Creating Linear Regression Class Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, fit_method='ols', loss_function=\"rmse\", l1=0, l2=0, learning_rate=0.01, epochs=1000, min_step_size=0.001, gradient_descent='batch', batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the LinearRegression model with a specified fitting method.\n",
    "\n",
    "        Parameters:\n",
    "        - fit_method: The fitting method to use: \"ols\" for Ordinary Least Squares, \"gd\" for Gradient Descent.\n",
    "        - learning_rate: Learning rate for Gradient Descent.\n",
    "        - loss_function: Loss function to use. rmse for Root Mean Squared Error. Only Root Mean Squared Error is supported for now.\n",
    "        - l1: L1 regularization parameter.\n",
    "        - l2: L2 regularization parameter.\n",
    "        - epochs: Number of epochs for Gradient Descent.\n",
    "        - min_step_size: Minimum step size for Gradient Descent.\n",
    "        - gradient_descent: Type of gradient_descent. Possible values: \"batch\", \"stochastic\", \"mini-batch\". \n",
    "        - batch_size: Size of batch for mini-bactch gradient descent.\n",
    "\n",
    "        Notes: \n",
    "        - You cant use l1 regularization with ols because there is no closed form solution.\n",
    "        \"\"\"\n",
    "\n",
    "        # general parameters\n",
    "        self.fit_method = fit_method\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # regularization parameters\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        # gradient descent parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.min_step_size = min_step_size\n",
    "        self.gradient_descent = gradient_descent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # initialize weights to none\n",
    "        self.weights = None # W0 will be bias.\n",
    "    \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the loss function value for the given true and predicted values with respect to loss function type and regularization parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true: True target values.\n",
    "        - y_pred: Predicted target values.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == 'rmse':\n",
    "            loss = np.sqrt(np.mean((y_true - y_pred) ** 2)) + self.l1 * np.sum(np.abs(self.weights)) + self.l2 * np.sum(self.weights ** 2)\n",
    "        else:\n",
    "            raise ValueError(\"loss_function should be either 'mse' or 'mae'\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def calculate_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the gradient for the loss function for given X, y_true and y_pred values.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "        - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self._predict(X)\n",
    "\n",
    "        if self.loss_function == 'rmse':\n",
    "            loss_gradient = - X.T @ (y - y_pred) / (X.shape[0] * np.sqrt(np.mean((y - y_pred) ** 2))) + self.l1 * np.sign(self.weights) + 2 * self.l2 * self.weights\n",
    "        else:\n",
    "            raise ValueError(\"loss_function should be rmse.\")\n",
    "\n",
    "        return loss_gradient\n",
    "\n",
    "    def fit_ols(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def fit_gd(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def fit_gd_batch(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def fit_gd_stochastic(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def fit_gd_mini_batch(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data based on selected fit method.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "        - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "        \"\"\"\n",
    "\n",
    "        # Add bias terms coefficent to the X for easier bias term handling.\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        if self.fit_method == 'ols':\n",
    "            self.fit_ols(X, y)\n",
    "        elif self.fit_method == 'gd':\n",
    "            self.fit_gd(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"fit_method should be either 'ols' or 'gd'\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for given inputs.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input value array for prediction. Should be numpy array with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        - y: Predictions values for input array X. numpy array with shape (n_samples, )\n",
    "        \"\"\"\n",
    "\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "        \n",
    "        # Add bias terms coefficent to the X for prediction.\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        y = self._predict(X)\n",
    "        return y\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"\n",
    "        Helper method for gradient descent. Using self.predict add 1s for the biases.\n",
    "        \"\"\"\n",
    "        return X @ self.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Fit Method Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A- Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is taken from Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning-The MIT Press (2016).\\\n",
    "Given the gradient of the training Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "\\nabla_w \\text{MSE}_{\\text{train}} = 0 \\tag{5.6}\n",
    "$$\n",
    "\n",
    "This implies:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\left( \\frac{1}{m} \\| \\hat{y}^{(\\text{train})} - y^{(\\text{train})} \\|^2_2 \\right) = 0 \\tag{5.7}\n",
    "$$\n",
    "\n",
    "Expanding it:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\nabla_w \\| X^{(\\text{train})} w - y^{(\\text{train})} \\|^2_2 = 0 \\tag{5.8}\n",
    "$$\n",
    "\n",
    "Taking the gradient with respect to \\( w \\):\n",
    "\n",
    "$$\n",
    "\\nabla_w \\left( X^{(\\text{train})} w - y^{(\\text{train})} \\right)^{\\top} \\left( X^{(\\text{train})} w - y^{(\\text{train})} \\right) = 0 \\tag{5.9}\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\left( w^{\\top} X^{(\\text{train})^{\\top}} X^{(\\text{train})} w - 2 w^{\\top} X^{(\\text{train})^{\\top}} y^{(\\text{train})} + y^{(\\text{train})^{\\top}} y^{(\\text{train})} \\right) = 0 \\tag{5.10}\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero:\n",
    "\n",
    "$$\n",
    "2 X^{(\\text{train})^{\\top}} X^{(\\text{train})} w - 2 X^{(\\text{train})^{\\top}} y^{(\\text{train})} = 0 \\tag{5.11}\n",
    "$$\n",
    "\n",
    "Solving for \\( w \\):\n",
    "\n",
    "$$\n",
    "w = \\left( X^{(\\text{train})^{\\top}} X^{(\\text{train})} \\right)^{-1} X^{(\\text{train})^{\\top}} y^{(\\text{train})} \\tag{5.12}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ols(self, X, y):\n",
    "    \"\"\"\n",
    "    Fit the model to the data using ordinary least squares fit method by calculating weights by given formula.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "    - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "    \"\"\"\n",
    "\n",
    "    self.weights = np.linalg.inv(X.T @ X + self.l2 * np.identity(X.shape[1])) @ X.T @ y\n",
    "\n",
    "# Assign it to the class method\n",
    "LinearRegression.fit_ols = fit_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B- Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a- Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd_batch(self, X, y):\n",
    "    \"\"\"\n",
    "    Fit the model to the data using batch gradient descent method by updating weights untill convergence.\n",
    "    Batch gradients use all the training data for updating weights at each step.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "    - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights\n",
    "    self.weights = np.random.randn(X.shape[1], ) * 0.01\n",
    "    self.weights[0] = 0 # Thats what they do in NN\n",
    "    \n",
    "    # Gradient Descent Loop\n",
    "    for _ in range(self.epochs):\n",
    "        gradient = self.calculate_gradient(X, y)\n",
    "        self.weights = self.weights - self.learning_rate * gradient\n",
    "\n",
    "# Assign it to the class method\n",
    "LinearRegression.fit_gd_batch = fit_gd_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd_stochastic(self, X, y):\n",
    "    \"\"\"\n",
    "    Fit the model to the data using batch gradient descent method by updating weights untill convergence.\n",
    "    Batch gradients use all the training data for updating weights at each step.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "    - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights\n",
    "    self.weights = np.random.randn(X.shape[1], ) * 0.01\n",
    "    self.weights[0] = 0 # Thats what they do in NN\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    current_index = 0\n",
    "    for epoch in range(self.epochs):\n",
    "        if epoch % n == 0:\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "        \n",
    "        current_X, current_y = X[current_index : current_index + 1], y[current_index]\n",
    "        current_index = (current_index + 1) % n\n",
    "        gradient = self.calculate_gradient(current_X, current_y)\n",
    "        self.weights = self.weights - self.learning_rate * gradient\n",
    "\n",
    "# Assign it to the class method\n",
    "LinearRegression.fit_gd_stochastic = fit_gd_stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c- Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd_mini_batch(self, X, y):\n",
    "    \"\"\"\n",
    "    Fit the model to the data using batch gradient descent method by updating weights untill convergence.\n",
    "    Batch gradients use all the training data for updating weights at each step.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input value array for training data. Should be numpy array with shape (n_samples, n_features).\n",
    "    - y: Target value array for training data. Should be numpy array with shape (n_samples, ).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights\n",
    "    self.weights = np.random.randn(X.shape[1], ) * 0.01\n",
    "    self.weights[0] = 0 # Thats what they do in NN\n",
    "\n",
    "    n = X.shape[0]\n",
    "    current_index = 0\n",
    "    for epoch in range(self.epochs):\n",
    "        if epoch % n == 0:\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "        \n",
    "        current_X, current_y = X[current_index : min(current_index + self.batch_size, n)], y[current_index : min(current_index + self.batch_size, n)]\n",
    "        current_index = min(current_index + self.batch_size, n) % n\n",
    "        gradient = self.calculate_gradient(current_X, current_y)\n",
    "        self.weights = self.weights - self.learning_rate * gradient\n",
    "\n",
    "# Assign it to the class method\n",
    "LinearRegression.fit_gd_mini_batch = fit_gd_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d- Merge all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd(self, X, y):\n",
    "    if self.gradient_descent == 'batch':\n",
    "        self.fit_gd_batch(X, y)\n",
    "    elif self.gradient_descent == 'stochastic':\n",
    "        self.fit_gd_stochastic(X, y)\n",
    "    elif self.gradient_descent == 'mini-batch':\n",
    "        self.fit_gd_mini_batch(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect gradient_descent value. Possible values: batch, stochastic, mini-batch.\")\n",
    "    \n",
    "# Assign it to the class method\n",
    "LinearRegression.fit_gd = fit_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Testing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A- Import and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,), (506, 13), (506,))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "\n",
    "X = np.array(boston.data).astype(float)\n",
    "y = np.array(boston.target).astype(float)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize train test\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "\n",
    "X_train_normalized = (X_train - X_train_mean) / X_train_std\n",
    "X_test_normalized = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B- Test OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a- Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.79653465 -1.00213533  0.69626862  0.27806485  0.7187384  -2.0223194\n",
      "  3.14523956 -0.17604788 -3.0819076   2.25140666 -1.76701378 -2.03775151\n",
      "  1.12956831 -3.61165842]\n",
      "OLS training rmse:  4.6520331848801675\n",
      "OLS test rmse:  4.928602182665336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "ols_model = LinearRegression(fit_method=\"ols\", loss_function=\"rmse\")\n",
    "ols_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "ols_pred_train = ols_model.predict(X_train_normalized)\n",
    "ols_rmse_train = root_mean_squared_error(y_train, ols_pred_train)\n",
    "\n",
    "ols_pred_test = ols_model.predict(X_test_normalized)\n",
    "ols_rmse_test = root_mean_squared_error(y_test, ols_pred_test)\n",
    "\n",
    "print(ols_model.weights)\n",
    "print(\"OLS training rmse: \", ols_rmse_train)\n",
    "print(\"OLS test rmse: \", ols_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c- L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.74024691 -0.99218679  0.6777488   0.2522143   0.72248078 -1.99083465\n",
      "  3.15157218 -0.17726162 -3.04502895  2.17324941 -1.69555879 -2.02783351\n",
      "  1.127197   -3.59897667]\n",
      "OLS with L2 regularization training rmse:  4.652517478423405\n",
      "OLS with L2 regularization test rmse:  4.933847995363045\n"
     ]
    }
   ],
   "source": [
    "ols_l2_model = LinearRegression(fit_method=\"ols\", loss_function=\"rmse\", l2=1)\n",
    "ols_l2_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "ols_l2_pred_train = ols_l2_model.predict(X_train_normalized)\n",
    "ols_l2_rmse_train = root_mean_squared_error(y_train, ols_l2_pred_train)\n",
    "\n",
    "ols_l2_pred_test = ols_l2_model.predict(X_test_normalized)\n",
    "ols_l2_rmse_test = root_mean_squared_error(y_test, ols_l2_pred_test)\n",
    "\n",
    "print(ols_l2_model.weights)\n",
    "print(\"OLS with L2 regularization training rmse: \", ols_l2_rmse_train)\n",
    "print(\"OLS with L2 regularization test rmse: \", ols_l2_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C- Test Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a- Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.7965344  -0.94343025  0.56883539  0.07435118  0.7470226  -1.90017958\n",
      "  3.21093155 -0.19545468 -2.96708916  1.63719496 -1.11664882 -2.00144587\n",
      "  1.1295799  -3.58772585]\n",
      "Batch gradient descent training rmse:  4.658384167500506\n",
      "Batch gradient descent test rmse:  4.975195483288953\n"
     ]
    }
   ],
   "source": [
    "batch_gd_model = LinearRegression(fit_method=\"gd\", loss_function=\"rmse\", gradient_descent='batch', epochs=10000)\n",
    "batch_gd_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "batch_gd_pred_train = batch_gd_model.predict(X_train_normalized)\n",
    "batch_gd_rmse_train = root_mean_squared_error(y_train, batch_gd_pred_train)\n",
    "\n",
    "batch_gd_pred_test = batch_gd_model.predict(X_test_normalized)\n",
    "batch_gd_rmse_test = root_mean_squared_error(y_test, batch_gd_pred_test)\n",
    "\n",
    "print(batch_gd_model.weights)\n",
    "print(\"Batch gradient descent training rmse: \", batch_gd_rmse_train)\n",
    "print(\"Batch gradient descent test rmse: \", batch_gd_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.92       -1.20788957  0.6041245   0.26417322  0.30123354 -0.89498717\n",
      "  4.2379923  -1.08926357 -2.1422088   0.69671495 -0.87866294 -1.4433898\n",
      "  1.24435545 -2.11143699]\n",
      "Stochastic gradient descent training rmse:  4.953760561452936\n",
      "Stochastic gradient descent test rmse:  5.40076021642857\n"
     ]
    }
   ],
   "source": [
    "stoc_gd_model = LinearRegression(fit_method=\"gd\", loss_function=\"rmse\", gradient_descent='stochastic', epochs=10000)\n",
    "stoc_gd_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "stoc_gd_pred_train = stoc_gd_model.predict(X_train_normalized)\n",
    "stoc_gd_rmse_train = root_mean_squared_error(y_train, stoc_gd_pred_train)\n",
    "\n",
    "stoc_gd_pred_test = stoc_gd_model.predict(X_test_normalized)\n",
    "stoc_gd_rmse_test = root_mean_squared_error(y_test, stoc_gd_pred_test)\n",
    "\n",
    "print(stoc_gd_model.weights)\n",
    "print(\"Stochastic gradient descent training rmse: \", stoc_gd_rmse_train)\n",
    "print(\"Stochastic gradient descent test rmse: \", stoc_gd_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c- Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.70491275 -0.93568033  0.53116471  0.07266191  0.59393139 -1.73026602\n",
      "  3.45927782 -0.405116   -2.85411164  1.46806366 -1.19012204 -1.93893251\n",
      "  1.10722116 -3.26944264]\n",
      "Stochastic gradient descent training rmse:  4.6730044174981025\n",
      "Stochastic gradient descent test rmse:  5.025684219881524\n"
     ]
    }
   ],
   "source": [
    "minibatch_gd_model = LinearRegression(fit_method=\"gd\", loss_function=\"rmse\", gradient_descent='mini-batch', epochs=10000, batch_size=32)\n",
    "minibatch_gd_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "minibatch_gd_pred_train = minibatch_gd_model.predict(X_train_normalized)\n",
    "minibatch_gd_rmse_train = root_mean_squared_error(y_train, minibatch_gd_pred_train)\n",
    "\n",
    "minibatch_gd_pred_test = minibatch_gd_model.predict(X_test_normalized)\n",
    "minibatch_gd_rmse_test = root_mean_squared_error(y_test, minibatch_gd_pred_test)\n",
    "\n",
    "print(minibatch_gd_model.weights)\n",
    "print(\"Stochastic gradient descent training rmse: \", minibatch_gd_rmse_train)\n",
    "print(\"Stochastic gradient descent test rmse: \", minibatch_gd_rmse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
